
// Servi√ßo para integra√ß√£o com Ollama local
const OLLAMA_API_URL = 'http://localhost:11434/api/generate';
const DEFAULT_MODEL = 'llama3.2:3b'; // Modelo padr√£o

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OllamaResponse {
  response: string;
  done: boolean;
}

export class OllamaService {
  private model: string;

  constructor(model?: string) {
    this.model = model || DEFAULT_MODEL;
  }

  async generateResponse(
    userMessage: string, 
    context: string, 
    chatbotName: string = 'Assistente'
  ): Promise<string> {
    console.log(`ü§ñ Ollama - Gerando resposta com modelo: ${this.model}`);
    console.log(`üë§ Usu√°rio: "${userMessage}"`);
    console.log(`ü§ñ Chatbot: ${chatbotName}`);
    console.log(`üìö Contexto (${context.length} caracteres):`, context.substring(0, 300) + '...');
    
    try {
      // Sistema de prompt que incorpora o conhecimento treinado
      const systemPrompt = `Voc√™ √© ${chatbotName}, um assistente virtual inteligente.
      
CONHECIMENTO BASE:
${context}

INSTRU√á√ïES:
- Use APENAS as informa√ß√µes fornecidas no conhecimento base para responder
- Se a pergunta n√£o puder ser respondida com as informa√ß√µes dispon√≠veis, diga: "Desculpe, n√£o tenho informa√ß√µes suficientes para responder essa pergunta. Posso ajud√°-lo com algo mais?"
- Seja cordial, profissional e direto
- Mantenha respostas concisas mas completas
- Se apropriado, ofere√ßa informa√ß√µes relacionadas que possam ser √∫teis`;

      const fullPrompt = `${systemPrompt}\n\nUsu√°rio: ${userMessage}\nAssistente:`;
      
      console.log(`üìù Prompt completo (${fullPrompt.length} caracteres):`, fullPrompt.substring(0, 500) + '...');

      const requestBody = {
        model: this.model,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.7,
          top_p: 0.9,
          max_tokens: 500
        }
      };
      
      console.log(`üöÄ Enviando requisi√ß√£o para Ollama:`, requestBody);

      const response = await fetch(OLLAMA_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      console.log(`üì° Resposta HTTP: ${response.status} ${response.statusText}`);

      if (!response.ok) {
        throw new Error(`Erro na API Ollama: ${response.status}`);
      }

      const data: OllamaResponse = await response.json();
      console.log(`‚úÖ Resposta do Ollama:`, data);
      
      const finalResponse = data.response || 'Desculpe, n√£o consegui gerar uma resposta.';
      console.log(`üì§ Resposta final: "${finalResponse}"`);
      
      return finalResponse;

    } catch (error) {
      console.error('‚ùå Erro ao chamar Ollama:', error);
      
      // Verificar se √© erro de conex√£o
      if (error instanceof TypeError && error.message.includes('fetch')) {
        return 'Erro: Ollama n√£o est√° rodando. Execute "ollama serve" no terminal e tente novamente.';
      }
      
      return 'Desculpe, estou temporariamente indispon√≠vel. Verifique se o Ollama est√° rodando.';
    }
  }

  // M√©todo para processar e extrair texto de documentos
  async processDocument(content: string, fileName: string): Promise<string> {
    console.log(`üìÑ Processando documento: ${fileName} (${content.length} caracteres)`);
    
    try {
      const prompt = `Extraia e organize as informa√ß√µes mais importantes deste documento. Mantenha a formata√ß√£o clara e organize por t√≥picos quando apropriado.

Documento: ${fileName}

Conte√∫do:
${content}

Resposta organizada:`;

      console.log(`üìù Prompt para processamento: ${prompt.substring(0, 200)}...`);

      const response = await fetch(OLLAMA_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.model,
          prompt: prompt,
          stream: false,
          options: {
            temperature: 0.3,
            max_tokens: 1000
          }
        })
      });

      if (!response.ok) {
        console.log(`‚ö†Ô∏è Erro HTTP ao processar documento: ${response.status}, usando conte√∫do original`);
        return content; // Retorna conte√∫do original se falhar
      }

      const data: OllamaResponse = await response.json();
      const processedContent = data.response || content;
      
      console.log(`‚úÖ Documento processado: ${processedContent.length} caracteres`);
      return processedContent;

    } catch (error) {
      console.error('‚ùå Erro ao processar documento:', error);
      console.log(`‚ö†Ô∏è Retornando conte√∫do original devido ao erro`);
      return content; // Retorna conte√∫do original se falhar
    }
  }

  // M√©todo para verificar se o Ollama est√° rodando
  async checkHealth(): Promise<boolean> {
    try {
      console.log(`üîç Verificando sa√∫de do Ollama...`);
      const response = await fetch('http://localhost:11434/api/tags');
      const isHealthy = response.ok;
      console.log(`üíö Ollama health check: ${isHealthy ? 'OK' : 'FALHOU'}`);
      return isHealthy;
    } catch (error) {
      console.error('‚ùå Erro no health check do Ollama:', error);
      return false;
    }
  }

  // M√©todo para listar modelos dispon√≠veis
  async listModels(): Promise<string[]> {
    try {
      console.log(`üìã Listando modelos dispon√≠veis...`);
      const response = await fetch('http://localhost:11434/api/tags');
      if (!response.ok) {
        console.log(`‚ö†Ô∏è Erro ao listar modelos: ${response.status}`);
        return [];
      }
      
      const data = await response.json();
      const models = data.models?.map((model: any) => model.name) || [];
      console.log(`üìä Modelos encontrados:`, models);
      return models;
    } catch (error) {
      console.error('‚ùå Erro ao listar modelos:', error);
      return [];
    }
  }
}

export const ollamaService = new OllamaService();
