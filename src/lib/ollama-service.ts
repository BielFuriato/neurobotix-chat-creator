
// Servi√ßo para integra√ß√£o com Ollama local
const OLLAMA_API_URL = 'http://localhost:11434/api/generate';
const DEFAULT_MODEL = 'llama3.2:3b'; // Modelo padr√£o

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OllamaResponse {
  response: string;
  done: boolean;
}

export class OllamaService {
  private model: string;

  constructor(model?: string) {
    this.model = model || DEFAULT_MODEL;
  }

  async generateResponse(
    userMessage: string, 
    context: string, 
    chatbotName: string = 'Assistente'
  ): Promise<string> {
    console.log(`ü§ñ Ollama - Gerando resposta com modelo: ${this.model}`);
    console.log(`üë§ Usu√°rio: "${userMessage}"`);
    console.log(`ü§ñ Chatbot: ${chatbotName}`);
    console.log(`üìö Contexto (${context.length} caracteres):`, context.substring(0, 500) + '...');
    
    try {
      // Sistema de prompt melhorado para usar o conhecimento de forma mais efetiva
      const systemPrompt = `Voc√™ √© ${chatbotName}, um assistente virtual especializado e inteligente.

IMPORTANTE: Use APENAS as informa√ß√µes da BASE DE CONHECIMENTO abaixo para responder. N√ÉO invente informa√ß√µes.

BASE DE CONHECIMENTO:
${context}

INSTRU√á√ïES DE RESPOSTA:
1. Leia cuidadosamente a pergunta do usu√°rio
2. Procure na base de conhecimento acima por informa√ß√µes relevantes
3. Se encontrar informa√ß√µes relevantes, responda de forma completa e precisa
4. Se N√ÉO encontrar informa√ß√µes na base de conhecimento, responda: "Desculpe, n√£o tenho informa√ß√µes espec√≠ficas sobre isso na minha base de conhecimento atual. Posso ajud√°-lo com algo mais relacionado ao que foi treinado?"
5. Seja cordial, profissional e direto
6. Use apenas fatos da base de conhecimento, nunca invente informa√ß√µes
7. Se apropriado, cite o documento de origem da informa√ß√£o

REGRAS IMPORTANTES:
- NUNCA invente pre√ßos, hor√°rios, pol√≠ticas ou informa√ß√µes n√£o presentes na base
- SEMPRE baseie suas respostas no conhecimento fornecido
- Se a pergunta for muito vaga, pe√ßa esclarecimentos
- Mantenha um tom profissional e prestativo`;

      const fullPrompt = `${systemPrompt}

PERGUNTA DO USU√ÅRIO: ${userMessage}

RESPOSTA (baseada apenas na base de conhecimento):`;
      
      console.log(`üìù Prompt completo gerado (${fullPrompt.length} caracteres)`);

      const requestBody = {
        model: this.model,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.3, // Mais baixo para respostas mais precisas
          top_p: 0.8,
          max_tokens: 800,
          stop: ['PERGUNTA DO USU√ÅRIO:', 'BASE DE CONHECIMENTO:']
        }
      };
      
      console.log(`üöÄ Enviando requisi√ß√£o para Ollama...`);

      const response = await fetch(OLLAMA_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      console.log(`üì° Resposta HTTP: ${response.status} ${response.statusText}`);

      if (!response.ok) {
        throw new Error(`Erro na API Ollama: ${response.status}`);
      }

      const data: OllamaResponse = await response.json();
      console.log(`‚úÖ Resposta bruta do Ollama:`, data);
      
      const finalResponse = data.response?.trim() || 'Desculpe, n√£o consegui gerar uma resposta adequada.';
      console.log(`üì§ Resposta final: "${finalResponse}"`);
      
      return finalResponse;

    } catch (error) {
      console.error('‚ùå Erro ao chamar Ollama:', error);
      
      // Verificar se √© erro de conex√£o
      if (error instanceof TypeError && error.message.includes('fetch')) {
        return 'Erro: Ollama n√£o est√° rodando. Execute "ollama serve" no terminal e tente novamente.';
      }
      
      return 'Desculpe, estou temporariamente indispon√≠vel. Verifique se o Ollama est√° rodando.';
    }
  }

  // M√©todo para processar e extrair texto de documentos
  async processDocument(content: string, fileName: string): Promise<string> {
    console.log(`üìÑ Processando documento: ${fileName} (${content.length} caracteres)`);
    
    try {
      const prompt = `Extraia e organize as informa√ß√µes mais importantes deste documento. Mantenha a formata√ß√£o clara e organize por t√≥picos quando apropriado.

Documento: ${fileName}

Conte√∫do:
${content}

Resposta organizada:`;

      console.log(`üìù Prompt para processamento: ${prompt.substring(0, 200)}...`);

      const response = await fetch(OLLAMA_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.model,
          prompt: prompt,
          stream: false,
          options: {
            temperature: 0.3,
            max_tokens: 1000
          }
        })
      });

      if (!response.ok) {
        console.log(`‚ö†Ô∏è Erro HTTP ao processar documento: ${response.status}, usando conte√∫do original`);
        return content; // Retorna conte√∫do original se falhar
      }

      const data: OllamaResponse = await response.json();
      const processedContent = data.response || content;
      
      console.log(`‚úÖ Documento processado: ${processedContent.length} caracteres`);
      return processedContent;

    } catch (error) {
      console.error('‚ùå Erro ao processar documento:', error);
      console.log(`‚ö†Ô∏è Retornando conte√∫do original devido ao erro`);
      return content; // Retorna conte√∫do original se falhar
    }
  }

  // M√©todo para verificar se o Ollama est√° rodando
  async checkHealth(): Promise<boolean> {
    try {
      console.log(`üîç Verificando sa√∫de do Ollama...`);
      const response = await fetch('http://localhost:11434/api/tags');
      const isHealthy = response.ok;
      console.log(`üíö Ollama health check: ${isHealthy ? 'OK' : 'FALHOU'}`);
      return isHealthy;
    } catch (error) {
      console.error('‚ùå Erro no health check do Ollama:', error);
      return false;
    }
  }

  // M√©todo para listar modelos dispon√≠veis
  async listModels(): Promise<string[]> {
    try {
      console.log(`üìã Listando modelos dispon√≠veis...`);
      const response = await fetch('http://localhost:11434/api/tags');
      if (!response.ok) {
        console.log(`‚ö†Ô∏è Erro ao listar modelos: ${response.status}`);
        return [];
      }
      
      const data = await response.json();
      const models = data.models?.map((model: any) => model.name) || [];
      console.log(`üìä Modelos encontrados:`, models);
      return models;
    } catch (error) {
      console.error('‚ùå Erro ao listar modelos:', error);
      return [];
    }
  }
}

export const ollamaService = new OllamaService();
